defaults:
  - _self_
  - task: square
  # - task: pickcan
  # - task: twoArmLift
  # - task: pushT
  # - task: metaworld_hammer
  # - task: line_following

task_name: ${task.task_name}
experiment_id: ${AGENT.experiment_id}

GENERAL:
  # simulator and tasks
  environment: ${task.environment}
  use_abs_action: ${task.use_abs_action}
  task: ${task.task_name}

  save_results: true
  
  number_of_repetitions: ${task.number_of_repetitions}
  evaluations_per_training: ${task.evaluations_per_training}
  max_num_of_episodes: ${task.max_num_of_episodes}
  max_time_steps_episode: ${task.max_time_steps_episode}
  max_time_steps_per_repetition: ${task.max_time_steps_per_repetition}

  # feedback
  theta: 0.1  # threshold for each dimension of the oracle teacher's action
  feedback_threshold: ${task.feedback_threshold}
  oracle_teacher: true
  human_teacher: false
  oracle_teaching_how_offen: 2
  executed_human_correction: true
  
  
  # adding noise to the teacher action
  oracle_teacher_noise: 0   # directional noise angle
  if_biased_directional_noise: false
  oracle_teacher_magnitude_noise: false 
  oracle_teacher_magnitude_noise_parameter: 0 # directional noise magnitude
  # oracle_teacher_only_one_dim: true  # paritial feedback or not, only tested for TwoArmLift task
  oracle_teacher_only_one_dim: false
  # oracle_teacher_Gaussian_noise: false    # Gaussian noise
  oracle_teacher_Gaussian_noise: true    # Gaussian noise

  Ta: 1  # horizon
  Ta_executed: 1

AGENT:
  use_abs_action: ${task.use_abs_action}
  
  image_crop_shape: ${task.crop_shape} # used in obs_encoder

  shape_meta: ${task.shape_meta}

  dim_a: ${task.dim_a}
  dim_o: ${task.dim_o}  # TODO define this in task.yaml

  e: ${task.e} # only define diagional 
  loss_weight_inverse_e: ${task.loss_weight_inverse_e}
  policy_model_learning_rate: 0.0003
  # number_training_iterations: 3000
  number_training_iterations: 1000

  Ta: ${GENERAL.Ta}  # horizon
  Ta_executed:  ${GENERAL.Ta_executed}

  agent: Diffusion
  algorithm: Diffusion_policy
  # algorithm: Diffusion_policy_relative

  use_ambient_loss: true  # loss for Ambient Diffusion for linear corrupted data
  ambient_k: 3  # number of dimensions that should trust; for PushT, 1, for Square, 3 or 4, for TwoArm, 7

  # DDPM
  DDPM_num_train_timesteps: 100

  # Buffer
  buffer_max_size: 35000
  buffer_min_size: 10
  buffer_sampling_rate: 5
  train_end_episode: true
  buffer_sampling_size: 10
  action_upper_limits: 1
  action_lower_limits: -1
  
  # log
  save_policy: true
  save_buffer: false
  experiment_id: "${now:%Y%m%d_%H%M%S}_${AGENT.algorithm}_${task.name}_Ambient"
  base_dir: saved_data/
  saved_dir: saved_data/
  load_policy: false
  # evaluate the policy under saved_dir
  evaluate: false
  load_dir: saved_data/
  
  # offline_training: true
  offline_training: false
  # if offline_training is true, load the buffer from the dir below
  load_pretrained_dir: outputs/
  
FEEDBACK:
  use_space_mouse: false
  key_type: 1
  h_up: 0
  h_down: 0
  h_right: 1
  h_left: -1
  h_null: 0
